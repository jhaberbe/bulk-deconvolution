{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ddba03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/jhaberbe/Projects/Personal/bulk-deconvolution/notebook/../src/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from tqdm import tqdm, trange\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import pseudobulk\n",
    "importlib.reload(pseudobulk)\n",
    "import models\n",
    "importlib.reload(models)\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcb8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "pbulk = sc.read_h5ad(\n",
    "    \"/home/jhaberbe/Projects/Personal/bulk-deconvolution/data/pbulk.h5ad\"\n",
    ")\n",
    "\n",
    "# Maybe exclude subset\n",
    "sc.pp.highly_variable_genes(pbulk, flavor=\"seurat_v3\", subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a73ec",
   "metadata": {},
   "source": [
    "# Training the mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e4e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "X_raw = torch.tensor(pbulk.X, dtype=torch.float32)  # (n, F), convert from sparse\n",
    "y = torch.tensor(pbulk.obs.values, dtype=torch.float32)\n",
    "y = y / y.sum(dim=1, keepdim=True).clamp(min=1e-8)  # Normalize rows\n",
    "\n",
    "X = utils.scanpy_log_normalize(X_raw)\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "mixture_model = models.MixturePrediction(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e837938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Train Loss: 39.8297 | Test Loss: 1.7247\n",
      "Epoch 01 | Train Loss: 11.4133 | Test Loss: 1.2601\n",
      "Epoch 02 | Train Loss: 8.2495 | Test Loss: 1.0629\n",
      "Epoch 03 | Train Loss: 7.2971 | Test Loss: 0.8544\n",
      "Epoch 04 | Train Loss: 6.2426 | Test Loss: 0.7956\n",
      "Epoch 05 | Train Loss: 5.4839 | Test Loss: 0.6319\n",
      "Epoch 06 | Train Loss: 4.8126 | Test Loss: 0.6221\n",
      "Epoch 07 | Train Loss: 4.5027 | Test Loss: 0.5177\n",
      "Epoch 08 | Train Loss: 3.9968 | Test Loss: 0.5271\n",
      "Epoch 09 | Train Loss: 3.7191 | Test Loss: 0.4592\n",
      "Epoch 10 | Train Loss: 3.4107 | Test Loss: 0.4445\n",
      "Epoch 11 | Train Loss: 3.3774 | Test Loss: 0.4286\n",
      "Epoch 12 | Train Loss: 3.0473 | Test Loss: 0.4444\n",
      "Epoch 13 | Train Loss: 3.0061 | Test Loss: 0.3754\n",
      "Epoch 14 | Train Loss: 2.8171 | Test Loss: 0.3558\n",
      "Epoch 15 | Train Loss: 2.8839 | Test Loss: 0.3505\n",
      "Epoch 16 | Train Loss: 2.8389 | Test Loss: 0.4110\n",
      "Epoch 17 | Train Loss: 2.7281 | Test Loss: 0.3439\n",
      "Epoch 18 | Train Loss: 2.6644 | Test Loss: 0.3423\n",
      "Epoch 19 | Train Loss: 2.7263 | Test Loss: 0.4425\n",
      "Epoch 20 | Train Loss: 2.7207 | Test Loss: 0.4142\n",
      "Epoch 21 | Train Loss: 2.7324 | Test Loss: 0.3829\n",
      "Epoch 22 | Train Loss: 2.6315 | Test Loss: 0.3366\n",
      "Epoch 23 | Train Loss: 2.6142 | Test Loss: 0.3287\n",
      "Epoch 24 | Train Loss: 2.5400 | Test Loss: 0.4400\n",
      "Epoch 25 | Train Loss: 2.5317 | Test Loss: 0.3211\n",
      "Epoch 26 | Train Loss: 2.5523 | Test Loss: 0.3743\n",
      "Epoch 27 | Train Loss: 2.5884 | Test Loss: 0.3399\n",
      "Epoch 28 | Train Loss: 2.3487 | Test Loss: 0.3319\n",
      "Epoch 29 | Train Loss: 2.3555 | Test Loss: 0.3447\n",
      "Epoch 30 | Train Loss: 2.4374 | Test Loss: 0.3094\n",
      "Epoch 31 | Train Loss: 2.2571 | Test Loss: 0.3634\n",
      "Epoch 32 | Train Loss: 2.3137 | Test Loss: 0.2994\n",
      "Epoch 33 | Train Loss: 2.2280 | Test Loss: 0.3085\n",
      "Epoch 34 | Train Loss: 2.2515 | Test Loss: 0.3344\n",
      "Epoch 35 | Train Loss: 2.4003 | Test Loss: 0.2858\n",
      "Epoch 36 | Train Loss: 2.1792 | Test Loss: 0.3237\n",
      "Epoch 37 | Train Loss: 2.1860 | Test Loss: 0.3527\n",
      "Epoch 38 | Train Loss: 2.2533 | Test Loss: 0.2880\n",
      "Epoch 39 | Train Loss: 2.0940 | Test Loss: 0.2998\n",
      "Epoch 40 | Train Loss: 2.2311 | Test Loss: 0.2981\n",
      "Epoch 41 | Train Loss: 2.0739 | Test Loss: 0.3701\n",
      "Epoch 42 | Train Loss: 2.0778 | Test Loss: 0.2801\n",
      "Epoch 43 | Train Loss: 2.0920 | Test Loss: 0.2909\n",
      "Epoch 44 | Train Loss: 2.0795 | Test Loss: 0.2761\n",
      "Epoch 45 | Train Loss: 1.9887 | Test Loss: 0.3128\n",
      "Epoch 46 | Train Loss: 2.1835 | Test Loss: 0.3415\n",
      "Epoch 47 | Train Loss: 2.2008 | Test Loss: 0.2664\n",
      "Epoch 48 | Train Loss: 2.0034 | Test Loss: 0.3163\n",
      "Epoch 49 | Train Loss: 2.0307 | Test Loss: 0.3071\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(mixture_model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Training loop ---\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    mixture_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = mixture_model(xb)\n",
    "        loss = F.kl_div(preds.log(), yb, reduction='batchmean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on test\n",
    "    mixture_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for xb, yb in test_loader:\n",
    "            preds = mixture_model(xb)\n",
    "            test_loss += F.kl_div(preds.log(), yb, reduction='batchmean').item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {total_loss:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c9b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mixture_model, \"../models/mixture_model.pt\")\n",
    "mixture_model = torch.load(\"../models/mixture_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3c309",
   "metadata": {},
   "source": [
    "# Training Dirichlet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8636d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_weights = mixture_model(X)\n",
    "mixture_weights = mixture_weights.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8db5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_model = models.MixtureToDirichlet(num_components=8, num_features=X.shape[1]).to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(dirichlet_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1137ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counts = torch.tensor(np.stack([pbulk.layers[layer] for layer in pbulk.layers])).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import DirichletMultinomial\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = pbulk.shape[0]\n",
    "\n",
    "dataset = TensorDataset(mixture_weights, counts)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    dirichlet_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_mixture, batch_counts in loader:\n",
    "        batch_mixture = batch_mixture.to(\"cuda\")\n",
    "        batch_counts = batch_counts.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        alpha_pred = dirichlet_model(batch_mixture)  # [batch_size, C, F]\n",
    "        loss = dirichlet_model.dirichlet_multinomial_loss(alpha_pred, batch_counts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f3775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dirichlet_model, \"../models/dirichlet_model.pt\")\n",
    "dirichlet_model = torch.load(\"../models/dirichlet_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4ae12",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838ec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_mle_counts(alpha: torch.Tensor, total_counts: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        alpha: [n, C, F] - Dirichlet concentration parameters\n",
    "        total_counts: [n, F] - observed total counts per feature per sample\n",
    "\n",
    "    Returns:\n",
    "        estimated_counts: [n, C, F] - MLE count split per component\n",
    "    \"\"\"\n",
    "    # Dirichlet mean\n",
    "    alpha_sum = alpha.sum(dim=1, keepdim=True)  # [n, 1, F]\n",
    "    probs = alpha / alpha_sum  # [n, C, F]\n",
    "\n",
    "    # Broadcast total counts across components\n",
    "    estimated_counts = probs * total_counts.unsqueeze(1)  # [n, C, F]\n",
    "    return estimated_counts\n",
    "\n",
    "layer_counts = dirichlet_mle_counts(\n",
    "    dirichlet_model(batch_mixture),\n",
    "    total_counts=torch.tensor(pbulk.X).to(\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88a79494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>astrocyte</th>\n",
       "      <th>endothelial cell</th>\n",
       "      <th>ependymal cell</th>\n",
       "      <th>fibroblast</th>\n",
       "      <th>microglial cell</th>\n",
       "      <th>mural cell</th>\n",
       "      <th>neuron</th>\n",
       "      <th>oligodendrocyte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1212</td>\n",
       "      <td>77</td>\n",
       "      <td>41</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>167</td>\n",
       "      <td>26</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>181</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138</td>\n",
       "      <td>34</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>147</td>\n",
       "      <td>141</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>317</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>107</td>\n",
       "      <td>72</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>65</td>\n",
       "      <td>240</td>\n",
       "      <td>560</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>138</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>102</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>296</td>\n",
       "      <td>82</td>\n",
       "      <td>293</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>136</td>\n",
       "      <td>46</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>740</td>\n",
       "      <td>188</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>819</td>\n",
       "      <td>59</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>173</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>96</td>\n",
       "      <td>85</td>\n",
       "      <td>357</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>394</td>\n",
       "      <td>10</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      astrocyte  endothelial cell  ependymal cell  fibroblast  \\\n",
       "0             0              1212              77          41   \n",
       "1            34               167              26         112   \n",
       "2           138                34              65          15   \n",
       "3             2               317              43           0   \n",
       "4            26               129               1          91   \n",
       "...         ...               ...             ...         ...   \n",
       "9995          0               190               0           0   \n",
       "9996        102                38              15         296   \n",
       "9997        136                46              29          22   \n",
       "9998        819                59              25           9   \n",
       "9999         96                85             357          15   \n",
       "\n",
       "      microglial cell  mural cell  neuron  oligodendrocyte  \n",
       "0                  84           0     307              206  \n",
       "1                   3          92     181             1031  \n",
       "2                  34         147     141              113  \n",
       "3                  73         107      72              389  \n",
       "4                  65         240     560              369  \n",
       "...               ...         ...     ...              ...  \n",
       "9995              237         138      49               23  \n",
       "9996               82         293      93                1  \n",
       "9997                5         740     188              193  \n",
       "9998              173          13      48              389  \n",
       "9999               15         394      10              399  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbulk.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33512348",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(pbulk.obs.columns):\n",
    "    pbulk.layers[\"expected \" + layer] = layer_counts[:, i, :].cpu().detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
