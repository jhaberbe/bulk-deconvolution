{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ddba03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/jhaberbe/Projects/Personal/bulk-deconvolution/notebook/../src/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from tqdm import tqdm, trange\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import pseudobulk\n",
    "importlib.reload(pseudobulk)\n",
    "import models\n",
    "importlib.reload(models)\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcb8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "pbulk = sc.read_h5ad(\n",
    "    \"/home/jhaberbe/Projects/Personal/bulk-deconvolution/data/pbulk.h5ad\"\n",
    ")\n",
    "\n",
    "# Maybe exclude subset\n",
    "sc.pp.highly_variable_genes(pbulk, flavor=\"seurat_v3\", subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a73ec",
   "metadata": {},
   "source": [
    "# Training the mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e4e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "X_raw = torch.tensor(pbulk.X, dtype=torch.float32)  # (n, F), convert from sparse\n",
    "y = torch.tensor(pbulk.obs.values, dtype=torch.float32)\n",
    "y = y / y.sum(dim=1, keepdim=True).clamp(min=1e-8)  # Normalize rows\n",
    "\n",
    "X = utils.scanpy_log_normalize(X_raw)\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "mixture_model = models.MixturePrediction(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e837938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Train Loss: 39.8297 | Test Loss: 1.7247\n",
      "Epoch 01 | Train Loss: 11.4133 | Test Loss: 1.2601\n",
      "Epoch 02 | Train Loss: 8.2495 | Test Loss: 1.0629\n",
      "Epoch 03 | Train Loss: 7.2971 | Test Loss: 0.8544\n",
      "Epoch 04 | Train Loss: 6.2426 | Test Loss: 0.7956\n",
      "Epoch 05 | Train Loss: 5.4839 | Test Loss: 0.6319\n",
      "Epoch 06 | Train Loss: 4.8126 | Test Loss: 0.6221\n",
      "Epoch 07 | Train Loss: 4.5027 | Test Loss: 0.5177\n",
      "Epoch 08 | Train Loss: 3.9968 | Test Loss: 0.5271\n",
      "Epoch 09 | Train Loss: 3.7191 | Test Loss: 0.4592\n",
      "Epoch 10 | Train Loss: 3.4107 | Test Loss: 0.4445\n",
      "Epoch 11 | Train Loss: 3.3774 | Test Loss: 0.4286\n",
      "Epoch 12 | Train Loss: 3.0473 | Test Loss: 0.4444\n",
      "Epoch 13 | Train Loss: 3.0061 | Test Loss: 0.3754\n",
      "Epoch 14 | Train Loss: 2.8171 | Test Loss: 0.3558\n",
      "Epoch 15 | Train Loss: 2.8839 | Test Loss: 0.3505\n",
      "Epoch 16 | Train Loss: 2.8389 | Test Loss: 0.4110\n",
      "Epoch 17 | Train Loss: 2.7281 | Test Loss: 0.3439\n",
      "Epoch 18 | Train Loss: 2.6644 | Test Loss: 0.3423\n",
      "Epoch 19 | Train Loss: 2.7263 | Test Loss: 0.4425\n",
      "Epoch 20 | Train Loss: 2.7207 | Test Loss: 0.4142\n",
      "Epoch 21 | Train Loss: 2.7324 | Test Loss: 0.3829\n",
      "Epoch 22 | Train Loss: 2.6315 | Test Loss: 0.3366\n",
      "Epoch 23 | Train Loss: 2.6142 | Test Loss: 0.3287\n",
      "Epoch 24 | Train Loss: 2.5400 | Test Loss: 0.4400\n",
      "Epoch 25 | Train Loss: 2.5317 | Test Loss: 0.3211\n",
      "Epoch 26 | Train Loss: 2.5523 | Test Loss: 0.3743\n",
      "Epoch 27 | Train Loss: 2.5884 | Test Loss: 0.3399\n",
      "Epoch 28 | Train Loss: 2.3487 | Test Loss: 0.3319\n",
      "Epoch 29 | Train Loss: 2.3555 | Test Loss: 0.3447\n",
      "Epoch 30 | Train Loss: 2.4374 | Test Loss: 0.3094\n",
      "Epoch 31 | Train Loss: 2.2571 | Test Loss: 0.3634\n",
      "Epoch 32 | Train Loss: 2.3137 | Test Loss: 0.2994\n",
      "Epoch 33 | Train Loss: 2.2280 | Test Loss: 0.3085\n",
      "Epoch 34 | Train Loss: 2.2515 | Test Loss: 0.3344\n",
      "Epoch 35 | Train Loss: 2.4003 | Test Loss: 0.2858\n",
      "Epoch 36 | Train Loss: 2.1792 | Test Loss: 0.3237\n",
      "Epoch 37 | Train Loss: 2.1860 | Test Loss: 0.3527\n",
      "Epoch 38 | Train Loss: 2.2533 | Test Loss: 0.2880\n",
      "Epoch 39 | Train Loss: 2.0940 | Test Loss: 0.2998\n",
      "Epoch 40 | Train Loss: 2.2311 | Test Loss: 0.2981\n",
      "Epoch 41 | Train Loss: 2.0739 | Test Loss: 0.3701\n",
      "Epoch 42 | Train Loss: 2.0778 | Test Loss: 0.2801\n",
      "Epoch 43 | Train Loss: 2.0920 | Test Loss: 0.2909\n",
      "Epoch 44 | Train Loss: 2.0795 | Test Loss: 0.2761\n",
      "Epoch 45 | Train Loss: 1.9887 | Test Loss: 0.3128\n",
      "Epoch 46 | Train Loss: 2.1835 | Test Loss: 0.3415\n",
      "Epoch 47 | Train Loss: 2.2008 | Test Loss: 0.2664\n",
      "Epoch 48 | Train Loss: 2.0034 | Test Loss: 0.3163\n",
      "Epoch 49 | Train Loss: 2.0307 | Test Loss: 0.3071\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(mixture_model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Training loop ---\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    mixture_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = mixture_model(xb)\n",
    "        loss = F.kl_div(preds.log(), yb, reduction='batchmean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on test\n",
    "    mixture_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for xb, yb in test_loader:\n",
    "            preds = mixture_model(xb)\n",
    "            test_loss += F.kl_div(preds.log(), yb, reduction='batchmean').item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {total_loss:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c9b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mixture_model, \"../models/mixture_model.pt\")\n",
    "mixture_model = torch.load(\"../models/mixture_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3c309",
   "metadata": {},
   "source": [
    "# Training Dirichlet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8636d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_weights = mixture_model(X)\n",
    "mixture_weights = mixture_weights.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8db5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_model = models.MixtureToDirichlet(num_components=8, num_features=X.shape[1]).to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(dirichlet_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1137ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counts = torch.tensor(np.stack([pbulk.layers[layer] for layer in pbulk.layers])).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import DirichletMultinomial\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = pbulk.shape[0]\n",
    "\n",
    "dataset = TensorDataset(mixture_weights, counts)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    dirichlet_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_mixture, batch_counts in loader:\n",
    "        batch_mixture = batch_mixture.to(\"cuda\")\n",
    "        batch_counts = batch_counts.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        alpha_pred = dirichlet_model(batch_mixture)  # [batch_size, C, F]\n",
    "        loss = dirichlet_model.dirichlet_multinomial_loss(alpha_pred, batch_counts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3775f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'MixtureToDirichlet' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# torch.save(model, \"../models/dirichlet_model.pt\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dirichlet_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/dirichlet_model.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/bulk-deconvolution/.venv/lib/python3.12/site-packages/torch/serialization.py:1525\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1533\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/bulk-deconvolution/.venv/lib/python3.12/site-packages/torch/serialization.py:2114\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2113\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2114\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2115\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2117\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/bulk-deconvolution/.venv/lib/python3.12/site-packages/torch/serialization.py:2103\u001b[39m, in \u001b[36m_load.<locals>.UnpicklerWrapper.find_class\u001b[39m\u001b[34m(self, mod_name, name)\u001b[39m\n\u001b[32m   2101\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2102\u001b[39m mod_name = load_module_mapping.get(mod_name, mod_name)\n\u001b[32m-> \u001b[39m\u001b[32m2103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'MixtureToDirichlet' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"../models/dirichlet_model.pt\")\n",
    "dirichlet_model = torch.load(\"../models/dirichlet_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ec4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
